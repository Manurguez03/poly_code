{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f322f2f",
   "metadata": {},
   "source": [
    "# Inspect key merge identifiers\n",
    "We will examine representative yearly files from the betting-odds and engineered match datasets to confirm available key columns for merge: winner, loser, tournament.\n",
    "\n",
    "Steps:\n",
    "1. Load one year (2022) from each folder.\n",
    "2. Detect columns containing winner / loser / tourney tokens.\n",
    "3. Preview those columns.\n",
    "\n",
    "Keeping code minimal so we can iterate next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fb7230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected files:\n",
      "Odds: c:\\poly_code\\Cleaned_Data_Betting-odds\\cleaned_atp_2022.xlsx\n",
      "Engineered: c:\\poly_code\\Cleaned_engineered_Data_Atp_matches\\merged_matches_2022.xlsx\n",
      "\n",
      "Shapes -> odds: (2547, 17)  engineered: (2556, 33)\n",
      "\n",
      "Detected key columns (odds): {'winner': ['Winner', 'prob_winner'], 'loser': ['Loser', 'prob_loser'], 'tourney': ['Tournament']}\n",
      "Detected key columns (engineered): {'winner': ['winner_age', 'winner_ht', 'winner_ioc', 'winner_name', 'winner_seed'], 'loser': ['loser_age', 'loser_ht', 'loser_ioc', 'loser_name', 'loser_seed'], 'tourney': ['tourney_id', 'tourney_level', 'tourney_name']}\n",
      "\n",
      "Common identifier columns: []\n",
      "No common columns detected yet; may need name normalization next.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, re, pathlib\n",
    "\n",
    "# Root folders (adjust if paths differ)\n",
    "odds_dir = pathlib.Path(r\"c:\\poly_code\\Cleaned_Data_Betting-odds\")\n",
    "eng_dir = pathlib.Path(r\"c:\\poly_code\\Cleaned_engineered_Data_Atp_matches\")\n",
    "\n",
    "# Pick a common year (prefer 2022 if present)\n",
    "YEAR = 2022\n",
    "\n",
    "# Helper to select file containing year\n",
    "def pick_file(folder: pathlib.Path, year: int):\n",
    "    pat = re.compile(str(year))\n",
    "    for f in sorted(folder.iterdir()):\n",
    "        if f.is_file() and pat.search(f.name) and f.suffix in {'.xlsx', '.csv'} and not f.name.startswith('~$'):\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "odds_file = pick_file(odds_dir, YEAR)\n",
    "eng_file = pick_file(eng_dir, YEAR)\n",
    "print(\"Selected files:\")\n",
    "print(\"Odds:\", odds_file)\n",
    "print(\"Engineered:\", eng_file)\n",
    "\n",
    "# Load (Excel vs CSV auto-handled)\n",
    "def load(path):\n",
    "    if path is None: return None\n",
    "    if path.suffix == '.csv':\n",
    "        return pd.read_csv(path)\n",
    "    return pd.read_excel(path)\n",
    "\n",
    "odds_df = load(odds_file)\n",
    "eng_df = load(eng_file)\n",
    "print('\\nShapes -> odds:', None if odds_df is None else odds_df.shape, ' engineered:', None if eng_df is None else eng_df.shape)\n",
    "\n",
    "# Candidate column detectors\n",
    "KEY_TOKENS = {\n",
    "    'winner': ['winner'],\n",
    "    'loser': ['loser', 'player2'],\n",
    "    'tourney': ['tourney', 'event', 'tournament']\n",
    "}\n",
    "\n",
    "def find_cols(df, tokens):\n",
    "    if df is None: return []\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    found = set()\n",
    "    for t in tokens:\n",
    "        for lc, orig in cols_lower.items():\n",
    "            if t in lc:\n",
    "                found.add(orig)\n",
    "    return sorted(found)\n",
    "\n",
    "odds_keys = {k: find_cols(odds_df, v) for k, v in KEY_TOKENS.items()}\n",
    "eng_keys = {k: find_cols(eng_df, v) for k, v in KEY_TOKENS.items()}\n",
    "\n",
    "print('\\nDetected key columns (odds):', odds_keys)\n",
    "print('Detected key columns (engineered):', eng_keys)\n",
    "\n",
    "# Preview first few rows for overlapping identifiers\n",
    "common_preview_cols = sorted(set(sum(odds_keys.values(), [])) & set(sum(eng_keys.values(), [])))\n",
    "print('\\nCommon identifier columns:', common_preview_cols)\n",
    "if common_preview_cols and odds_df is not None and eng_df is not None:\n",
    "    print('\\nOdds sample:')\n",
    "    print(odds_df[common_preview_cols].head(3))\n",
    "    print('\\nEngineered sample:')\n",
    "    print(eng_df[common_preview_cols].head(3))\n",
    "else:\n",
    "    print('No common columns detected yet; may need name normalization next.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e4e96",
   "metadata": {},
   "source": [
    "# Normalize player names to a common key\n",
    "We will convert both datasets' winner/loser names to the format: `Last F` (last name + space + first initial, no period).\n",
    "This will help establish merge keys before tackling tournaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79004bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds name preview:\n",
      "        Winner  winner_key               Loser   loser_key\n",
      "0    Kwon S.W.      S.W. K         Nishioka Y.  Nishioka Y\n",
      "1  Monteiro T.  Monteiro T         Altmaier D.  Altmaier D\n",
      "2     Djere L.     Djere L  Carballes Baena R.        R. C\n",
      "3   Johnson S.   Johnson S            Vukic A.     Vukic A\n",
      "4    Moutet C.    Moutet C             Rune H.      Rune H\n",
      "\n",
      "Engineered name preview:\n",
      "             winner_name    winner_key             loser_name    loser_key\n",
      "0  Felix Auger Aliassime   Aliassime F  Roberto Bautista Agut       Agut R\n",
      "1       Denis Shapovalov  Shapovalov D    Pablo Carreno Busta      Busta P\n",
      "2  Roberto Bautista Agut        Agut R         Hubert Hurkacz    Hurkacz H\n",
      "3    Pablo Carreno Busta       Busta P          Jan Zielinski  Zielinski J\n",
      "4        Daniil Medvedev    Medvedev D  Felix Auger Aliassime  Aliassime F\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "PARTICLES = {\"de\", \"del\", \"de la\", \"van\", \"von\", \"da\", \"di\", \"dos\", \"du\", \"la\", \"le\"}\n",
    "\n",
    "# Basic cleaner\n",
    "def _clean(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s.strip())\n",
    "    s = s.replace(\" \", \" \")  # non-breaking space\n",
    "    return s\n",
    "\n",
    "# Detect already-normalized: \"Last F\" or \"Last F.\" patterns\n",
    "def _is_last_initial(s: str) -> bool:\n",
    "    parts = s.split()\n",
    "    return len(parts) == 2 and (len(parts[1]) == 1 or (len(parts[1]) == 2 and parts[1].endswith('.')))\n",
    "\n",
    "# Main normalization to \"Last F\"\n",
    "def normalize_name(x):\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)):\n",
    "        return None\n",
    "    s = _clean(str(x))\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # If already like \"Last F\" or \"Last F.\" -> ensure single space and drop period\n",
    "    if _is_last_initial(s):\n",
    "        last, ini = s.split()\n",
    "        ini = ini[0]\n",
    "        return f\"{last} {ini.upper()}\"\n",
    "\n",
    "    # Handle comma: \"Last, First Middle\"\n",
    "    if \",\" in s:\n",
    "        last, rest = s.split(\",\", 1)\n",
    "        last = _clean(last)\n",
    "        rest = _clean(rest)\n",
    "        first = rest.split()[0] if rest else \"\"\n",
    "        return f\"{last} {first[:1].upper()}\" if first else last\n",
    "\n",
    "    # No comma: try common patterns like \"First Middle Last\" or particles\n",
    "    tokens = s.split()\n",
    "    if len(tokens) == 1:\n",
    "        return tokens[0]\n",
    "\n",
    "    # Join two-token trailing surname if a known particle precedes the last token\n",
    "    if len(tokens) >= 3 and tokens[-2].lower() in PARTICLES:\n",
    "        last = tokens[-2] + \" \" + tokens[-1]\n",
    "        first = tokens[0]\n",
    "    else:\n",
    "        first = tokens[0]\n",
    "        last = tokens[-1]\n",
    "\n",
    "    return f\"{last} {first[:1].upper()}\"\n",
    "\n",
    "# Determine name columns in each df\n",
    "odds_w, odds_l = None, None\n",
    "for c in (odds_df.columns if isinstance(odds_df, pd.DataFrame) else []):\n",
    "    cl = c.lower()\n",
    "    if cl == 'winner' and odds_w is None:\n",
    "        odds_w = c\n",
    "    if cl == 'loser' and odds_l is None:\n",
    "        odds_l = c\n",
    "\n",
    "eng_w = 'winner_name' if isinstance(eng_df, pd.DataFrame) and 'winner_name' in eng_df.columns else None\n",
    "eng_l = 'loser_name' if isinstance(eng_df, pd.DataFrame) and 'loser_name' in eng_df.columns else None\n",
    "\n",
    "# Create normalized keys (only if columns exist)\n",
    "if isinstance(odds_df, pd.DataFrame) and odds_w and odds_l:\n",
    "    odds_df['winner_key'] = odds_df[odds_w].map(normalize_name)\n",
    "    odds_df['loser_key'] = odds_df[odds_l].map(normalize_name)\n",
    "\n",
    "if isinstance(eng_df, pd.DataFrame) and eng_w and eng_l:\n",
    "    eng_df['winner_key'] = eng_df[eng_w].map(normalize_name)\n",
    "    eng_df['loser_key'] = eng_df[eng_l].map(normalize_name)\n",
    "\n",
    "# Preview\n",
    "print(\"Odds name preview:\")\n",
    "if isinstance(odds_df, pd.DataFrame) and odds_w and odds_l:\n",
    "    print(odds_df[[odds_w, 'winner_key', odds_l, 'loser_key']].head(5))\n",
    "else:\n",
    "    print(\"(name columns not found in odds_df)\")\n",
    "\n",
    "print(\"\\nEngineered name preview:\")\n",
    "if isinstance(eng_df, pd.DataFrame) and eng_w and eng_l:\n",
    "    print(eng_df[[eng_w, 'winner_key', eng_l, 'loser_key']].head(5))\n",
    "else:\n",
    "    print(\"(name columns not found in eng_df)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603cb68",
   "metadata": {},
   "source": [
    "# Normalize tournament names to a common key\n",
    "We will create `tourney_key` in both datasets by:\n",
    "- Lowercasing and stripping punctuation/digits\n",
    "- Removing generic terms (e.g., \"Open\", \"Masters\", \"Cup\", \"ATP\", sponsors)\n",
    "- Returning the remaining tokens Title-Cased (e.g., \"Cordoba Open\" -> \"Cordoba\").\n",
    "This keeps things simple and aligns names across sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23dd37ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds tournament preview:\n",
      "                 Tournament    tourney_key\n",
      "0    Brisbane International       Brisbane\n",
      "29    Hong Kong Tennis Open      Hong Kong\n",
      "56   Adelaide International       Adelaide\n",
      "83              ASB Classic            Asb\n",
      "108         Australian Open     Australian\n",
      "231      Open Sud de France  Sud De France\n",
      "257            Cordoba Open        Cordoba\n",
      "283             Dallas Open         Dallas\n",
      "307                 Open 13           Open\n",
      "334          Argentina Open      Argentina\n",
      "\n",
      "Engineered tournament preview:\n",
      "        tourney_name  tourney_key\n",
      "0           Brisbane     Brisbane\n",
      "29        United Cup       United\n",
      "54         Hong Kong    Hong Kong\n",
      "81          Adelaide     Adelaide\n",
      "108         Auckland     Auckland\n",
      "133  Australian Open   Australian\n",
      "256      Montpellier  Montpellier\n",
      "282          Cordoba      Cordoba\n",
      "308           Dallas       Dallas\n",
      "332        Marseille    Marseille\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# Stopwords to drop from tournament names (keep minimal & generic)\n",
    "STOPWORDS = {\n",
    "    'open','masters','cup','championship','championships','tournament','atp','wta',\n",
    "    'presented','presentedby','presented-by','by','the','tennis','international','internationals',\n",
    "    'series','classic','bank','western','southern','rolex','mutua','bnp','paribas','rakuten','bet','betway',\n",
    "    'master','finals','final','event','tour','world','masters1000','1000','500','250'\n",
    "}\n",
    "\n",
    "ROMAN_RE = re.compile(r'^(?=[MDCLXVI])(M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))$', re.I)\n",
    "PUNCT_DIGIT_RE = re.compile(r\"[^A-Za-zÀ-ÖØ-öø-ÿ\\s]\")\n",
    "\n",
    "# French Open and Roland Garros equivalence\n",
    "FRENCH_EQUIV = {\"french open\", \"roland garros\"}\n",
    "\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
    "\n",
    "\n",
    "def normalize_tourney(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # lowercase, remove accents\n",
    "    s = strip_accents(s.lower())\n",
    "    # remove punctuation (keep spaces) and digits/symbols\n",
    "    s = PUNCT_DIGIT_RE.sub(' ', s)\n",
    "    s = re.sub(r\"\\d+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # Special rule: French Open and Roland Garros are the same\n",
    "    s_nosp = s.replace(' ', '')\n",
    "    if s in FRENCH_EQUIV or s_nosp in {w.replace(' ', '') for w in FRENCH_EQUIV}:\n",
    "        return 'Roland Garros'\n",
    "    if 'french' in s and 'open' in s:\n",
    "        return 'Roland Garros'\n",
    "    if 'roland' in s and 'garros' in s:\n",
    "        return 'Roland Garros'\n",
    "\n",
    "    tokens = [t for t in s.split(' ') if t]\n",
    "    keep = []\n",
    "    for t in tokens:\n",
    "        if t in STOPWORDS:\n",
    "            continue\n",
    "        if ROMAN_RE.match(t):\n",
    "            continue\n",
    "        keep.append(t)\n",
    "\n",
    "    if not keep:\n",
    "        # fallback: first token if everything stripped\n",
    "        keep = tokens[:1]\n",
    "\n",
    "    # Title-case the result for readability\n",
    "    return ' '.join(w.capitalize() for w in keep)\n",
    "\n",
    "# Determine tournament columns\n",
    "odds_t = None\n",
    "if isinstance(odds_df, pd.DataFrame):\n",
    "    for c in odds_df.columns:\n",
    "        if c.lower() == 'tournament':\n",
    "            odds_t = c\n",
    "            break\n",
    "\n",
    "eng_t = 'tourney_name' if isinstance(eng_df, pd.DataFrame) and 'tourney_name' in eng_df.columns else None\n",
    "\n",
    "# Apply normalization\n",
    "if isinstance(odds_df, pd.DataFrame) and odds_t:\n",
    "    odds_df['tourney_key'] = odds_df[odds_t].map(normalize_tourney)\n",
    "\n",
    "if isinstance(eng_df, pd.DataFrame) and eng_t:\n",
    "    eng_df['tourney_key'] = eng_df[eng_t].map(normalize_tourney)\n",
    "\n",
    "# Preview examples\n",
    "print('Odds tournament preview:')\n",
    "if isinstance(odds_df, pd.DataFrame) and odds_t:\n",
    "    print(odds_df[[odds_t, 'tourney_key']].drop_duplicates().head(10))\n",
    "else:\n",
    "    print('(tournament column not found in odds_df)')\n",
    "\n",
    "print('\\nEngineered tournament preview:')\n",
    "if isinstance(eng_df, pd.DataFrame) and eng_t:\n",
    "    print(eng_df[[eng_t, 'tourney_key']].drop_duplicates().head(10))\n",
    "else:\n",
    "    print('(tourney_name column not found in eng_df)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646cd351",
   "metadata": {},
   "source": [
    "# Merge datasets on normalized keys\n",
    "We will now merge the two datasets using the unique features:\n",
    "- winner_key\n",
    "- loser_key\n",
    "- tourney_key\n",
    "This will create a combined DataFrame for further modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b4490b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (1022, 53)\n",
      "    winner_key    loser_key tourney_key tourney_id tourney_name surface  draw_size tourney_level\n",
      "0   Dimitrov G       Rune H    Brisbane  2024-0339     Brisbane    Hard         32             A\n",
      "1       Rune H  Safiullin R    Brisbane  2024-0339     Brisbane    Hard         32             A\n",
      "2   Dimitrov G   Thompson J    Brisbane  2024-0339     Brisbane    Hard         32             A\n",
      "3       Rune H  Duckworth J    Brisbane  2024-0339     Brisbane    Hard         32             A\n",
      "4  Safiullin R    Arnaldi M    Brisbane  2024-0339     Brisbane    Hard         32             A\n"
     ]
    }
   ],
   "source": [
    "# Perform the merge on winner_key, loser_key, tourney_key\n",
    "merge_keys = ['winner_key', 'loser_key', 'tourney_key']\n",
    "\n",
    "# Only merge if both DataFrames are present and have the keys\n",
    "if all(isinstance(df, pd.DataFrame) for df in [odds_df, eng_df]):\n",
    "    missing_cols = [k for k in merge_keys if k not in odds_df.columns or k not in eng_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing merge columns: {missing_cols}\")\n",
    "        merged_df = None\n",
    "    else:\n",
    "        merged_df = pd.merge(\n",
    "            eng_df, odds_df,\n",
    "            on=merge_keys,\n",
    "            how='inner',\n",
    "            suffixes=('_eng', '_odds')\n",
    "        )\n",
    "        print(f\"Merged shape: {merged_df.shape}\")\n",
    "        print(merged_df[merge_keys + [col for col in merged_df.columns if col not in merge_keys][:5]].head())\n",
    "else:\n",
    "    print(\"One or both DataFrames missing; cannot merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f95953",
   "metadata": {},
   "source": [
    "# Clean Merged_By_Year files: drop repeated features\n",
    "We'll remove repeated columns from each yearly merged file to reduce duplication before further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56737236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_matches_2016.xlsx: removed 2 cols -> ['surface', 'round'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2016.xlsx\n",
      "merged_matches_2017.xlsx: removed 2 cols -> ['surface', 'round'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2017.xlsx\n",
      "merged_matches_2018.xlsx: removed 2 cols -> ['surface', 'round'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2018.xlsx\n",
      "merged_matches_2019.xlsx: removed 2 cols -> ['surface', 'round'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2019.xlsx\n",
      "merged_matches_2021.xlsx: removed 2 cols -> ['surface', 'round'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2021.xlsx\n",
      "merged_matches_2022.xlsx: removed 2 cols -> ['surface', 'round'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2022.xlsx\n",
      "merged_matches_2023.xlsx: removed 11 cols -> ['tourney_key', 'Tournament', 'Court', 'Surface', 'Location', 'Best of', 'winner_key', 'Winner', 'Round', 'Loser', 'loser_key'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2023.xlsx\n",
      "merged_matches_2024.xlsx: removed 11 cols -> ['tourney_key', 'Tournament', 'Court', 'Surface', 'Location', 'Best of', 'winner_key', 'Winner', 'Round', 'Loser', 'loser_key'] | saved: c:\\poly_code\\Merged_By_Year\\cleaned\\merged_matches_2024.xlsx\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "merged_dir = pathlib.Path(r\"c:\\poly_code\\Merged_By_Year\")\n",
    "output_dir = merged_dir / \"cleaned\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns to drop (case-insensitive match)\n",
    "DROP_COLS = {\n",
    "    'tourney_key', 'location', 'tournament',\n",
    "    'court', 'surface', 'round', 'best of',\n",
    "    'winner', 'loser', 'winner_key', 'loser_key'\n",
    "}\n",
    "\n",
    "removed_summary = []\n",
    "for f in sorted(merged_dir.iterdir()):\n",
    "    if not f.is_file() or f.name.startswith('~$') or f.suffix.lower() not in {'.xlsx', '.csv'}:\n",
    "        continue\n",
    "\n",
    "    # Load\n",
    "    if f.suffix.lower() == '.csv':\n",
    "        df = pd.read_csv(f)\n",
    "    else:\n",
    "        df = pd.read_excel(f)\n",
    "\n",
    "    # Map lower-case to original cols\n",
    "    lc_map = {str(c).strip().lower(): c for c in df.columns}\n",
    "    to_drop_real = [lc_map[k] for k in DROP_COLS if k in lc_map]\n",
    "\n",
    "    # Drop\n",
    "    if to_drop_real:\n",
    "        df = df.drop(columns=to_drop_real, errors='ignore')\n",
    "\n",
    "    # Save to cleaned folder (avoid in-place write locks)\n",
    "    out_path = output_dir / f.name\n",
    "    if f.suffix.lower() == '.csv':\n",
    "        df.to_csv(out_path, index=False)\n",
    "    else:\n",
    "        df.to_excel(out_path, index=False)\n",
    "\n",
    "    removed_summary.append((f.name, to_drop_real, out_path))\n",
    "\n",
    "# Print concise summary\n",
    "for name, cols, out in removed_summary:\n",
    "    print(f\"{name}: removed {len(cols)} cols -> {cols} | saved: {out}\")\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
